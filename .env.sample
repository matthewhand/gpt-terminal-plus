# Required environment variables
API_TOKEN=your_secure_api_token # Generate something HTTP friendly eg. `openssl rand -base64 48` 
NODE_CONFIG_DIR=./config # Read/write required. Used for persistent storage between docker instances. Optional configuration files go here.

# Optional environment variables. These can be added to .env if needed (individually if needed).
NGROK_ENABLED=true
NGROK_PORT=5005
NGROK_AUTHTOKEN=your-ngrok-authtoken

NODE_ENV= # production # Which config file to load (if any).

DEBUG= # true 

# Enables code execution functionality
ENABLE_CODE_EXECUTION=true # false

# Enable or disable command management routes.
ENABLE_COMMAND_MANAGEMENT= # false

# Enable or disable file management routes.
ENABLE_FILE_MANAGEMENT= # false

# Enable or disable server management routes.
ENABLE_SERVER_MANAGEMENT= # false

# Suppress warnings related to missing configuration.
SUPPRESS_NO_CONFIG_WARNING= # true

# Disable health log
DISABLE_HEALTH_LOG= # true

# Escaping options (if not done already by prompt)
ESCAPE_DOLLAR= # false
ESCAPE_BACKTICKS= # false
ESCAPE_BACKWARD_SLASH= # false
ESCAPE_FORWARD_SLASH= # false
ESCAPE_QUOTES= # false

# HTTPS Configuration (if not using nginx+certbot proxy script)
HTTPS_ENABLED= # false
HTTPS_KEY_PATH= # path/to/your/private.key
HTTPS_CERT_PATH= # path/to/your/certificate.crt

# Model selection and AI providers
DEFAULT_MODEL=gpt-oss:20b
AI_PROVIDER=ollama  # or lmstudio, openai
OLLAMA_BASE_URL=http://localhost:11434
LMSTUDIO_BASE_URL=http://localhost:1234
OPENAI_BASE_URL=https://api.openai.com
OPENAI_API_KEY=

# Streaming
SSE_HEARTBEAT_MS=15000

# MCP (optional)
USE_MCP= # true

# AI analysis on failures
AUTO_ANALYZE_ERRORS=true

# LLM Configuration (Optional - App works without AI)
# Set LLM_ENABLED=true to enable AI-powered features
LLM_ENABLED=false
LLM_PROVIDER= # openai | ollama | lmstudio | open-interpreter
LLM_DEFAULT_MODEL= # gpt-4o | llama2 | local-model

# Provider-specific settings (use ${...} placeholders for secrets)
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_API_KEY=${OPENAI_API_KEY} # Set your actual key in environment
OLLAMA_URL=http://localhost:11434
LM_STUDIO_URL=http://localhost:1234/v1

# Deprecated - Use LLM_* settings above instead
# AI_PROVIDER=ollama
# OLLAMA_BASE_URL=http://localhost:11434
# LMSTUDIO_BASE_URL=http://localhost:1234

# Execution Limits and Circuit Breakers
MAX_INPUT_CHARS=10000
MAX_OUTPUT_CHARS=200000
MAX_LLM_COST_USD=0 # Set to positive value to enable budget limits
ALLOW_TRUNCATION=true

# Security Settings
DENY_COMMAND_REGEX= # Comma-separated regex patterns to block
CONFIRM_COMMAND_REGEX= # Comma-separated regex patterns requiring confirmation
SHELL_ALLOWED= # Comma-separated list of allowed shells (bash,zsh,powershell)

# Executor Configuration
EXEC_EXPOSURE_MODE=generic # generic | specific | both | none
EXEC_BASH_ENABLED=true
EXEC_PYTHON_ENABLED=true
EXEC_TS_ENABLED=false
EXEC_POWERSHELL_ENABLED=false

# Timeout Settings (milliseconds)
EXEC_BASH_TIMEOUT_MS=0 # 0 = no timeout
EXEC_PYTHON_TIMEOUT_MS=0
SSH_TIMEOUT=30000
SSM_TIMEOUT=300000
