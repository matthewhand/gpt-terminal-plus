# Required environment variables
API_TOKEN=your_secure_api_token # Generate something HTTP friendly eg. `openssl rand -base64 48` 
NODE_CONFIG_DIR=./config # Read/write required. Used for persistent storage between docker instances. Optional configuration files go here.

# Optional environment variables. These can be added to .env if needed (individually if needed).
NGROK_ENABLED=true
NGROK_PORT=5005
NGROK_AUTHTOKEN=your-ngrok-authtoken

NODE_ENV= # production # Which config file to load (if any).

DEBUG= # true 

# Enables code execution functionality
ENABLE_CODE_EXECUTION=true # false

# Enable or disable command management routes.
ENABLE_COMMAND_MANAGEMENT= # false

# Enable or disable file management routes.
ENABLE_FILE_MANAGEMENT= # false

# Enable or disable server management routes.
ENABLE_SERVER_MANAGEMENT= # false

# Suppress warnings related to missing configuration.
SUPPRESS_NO_CONFIG_WARNING= # true

# Disable health log
DISABLE_HEALTH_LOG= # true

# Escaping options (if not done already by prompt)
ESCAPE_DOLLAR= # false
ESCAPE_BACKTICKS= # false
ESCAPE_BACKWARD_SLASH= # false
ESCAPE_FORWARD_SLASH= # false
ESCAPE_QUOTES= # false

# HTTPS Configuration (if not using nginx+certbot proxy script)
HTTPS_ENABLED= # false
HTTPS_KEY_PATH= # path/to/your/private.key
HTTPS_CERT_PATH= # path/to/your/certificate.crt

# LLM configuration (optional)
LLM_ENABLED=false # set to true to enable LLM features
LLM_PROVIDER=none # options: ollama, lmstudio, openai
DEFAULT_MODEL=gpt-oss:20b # logical model when LLM is enabled

# Provider-specific settings
OLLAMA_BASE_URL=http://localhost:11434 # used when LLM_PROVIDER=ollama
LMSTUDIO_BASE_URL=http://localhost:1234 # used when LLM_PROVIDER=lmstudio
OPENAI_BASE_URL=https://api.openai.com # OpenAI or LiteLLM proxy
OPENAI_API_KEY= # required when LLM_PROVIDER=openai

# Streaming
SSE_HEARTBEAT_MS=15000

# MCP (optional)
USE_MCP= # true

# AI analysis on failures
AUTO_ANALYZE_ERRORS=true # silently ignored when LLM_ENABLED=false
